{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MIMIC-IV ED dataset\n",
      "Loading edstays...\n",
      "Table loaded: edstays\n",
      "Loading demographics...\n",
      "Table loaded: edstays\n",
      "Table loaded: age\n",
      "Loading age data...\n",
      "Table loaded: patients\n",
      "Calculating ED visit age...\n",
      "Merging time features...\n",
      "Table loaded: edstays\n",
      "Merging triage features...\n",
      "Table loaded: triage\n",
      "Cleaning up columns...\n",
      "\n",
      " Dataframe shape: (425087, 18) \n",
      "\n",
      "Dataframe info: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 425087 entries, 0 to 425086\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   subject_id         425087 non-null  Int64  \n",
      " 1   stay_id            425087 non-null  Int64  \n",
      " 2   gender             425087 non-null  object \n",
      " 3   arrival_transport  425087 non-null  object \n",
      " 4   disposition        425087 non-null  object \n",
      " 5   race               425087 non-null  object \n",
      " 6   age_at_ed          425087 non-null  Int64  \n",
      " 7   dow                425087 non-null  object \n",
      " 8   hour               425087 non-null  int32  \n",
      " 9   minute             425087 non-null  int32  \n",
      " 10  temperature        401672 non-null  Float64\n",
      " 11  heartrate          407997 non-null  Float64\n",
      " 12  resprate           404734 non-null  Float64\n",
      " 13  o2sat              404491 non-null  Float64\n",
      " 14  sbp                406796 non-null  Float64\n",
      " 15  dbp                405996 non-null  Float64\n",
      " 16  pain               396971 non-null  Int64  \n",
      " 17  acuity             418100 non-null  Int64  \n",
      "dtypes: Float64(6), Int64(5), int32(2), object(5)\n",
      "memory usage: 59.6+ MB\n",
      "None\n",
      "Verifying no duplicate stay_ids...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>arrival_transport</th>\n",
       "      <th>disposition</th>\n",
       "      <th>race</th>\n",
       "      <th>age_at_ed</th>\n",
       "      <th>dow</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155086</th>\n",
       "      <td>17073405</td>\n",
       "      <td>32725441</td>\n",
       "      <td>F</td>\n",
       "      <td>WALK IN</td>\n",
       "      <td>HOME</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>51</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>98.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133265</th>\n",
       "      <td>19712154</td>\n",
       "      <td>34421659</td>\n",
       "      <td>M</td>\n",
       "      <td>WALK IN</td>\n",
       "      <td>HOME</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>55</td>\n",
       "      <td>Friday</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>96.5</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92830</th>\n",
       "      <td>16280495</td>\n",
       "      <td>34330125</td>\n",
       "      <td>M</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>HOME</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>49</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>98.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357427</th>\n",
       "      <td>19944585</td>\n",
       "      <td>35033215</td>\n",
       "      <td>F</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>85</td>\n",
       "      <td>Friday</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>99.6</td>\n",
       "      <td>65.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316567</th>\n",
       "      <td>15777013</td>\n",
       "      <td>30740732</td>\n",
       "      <td>M</td>\n",
       "      <td>WALK IN</td>\n",
       "      <td>ADMITTED</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>70</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>97.8</td>\n",
       "      <td>58.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id   stay_id gender arrival_transport disposition   race  \\\n",
       "155086    17073405  32725441      F           WALK IN        HOME  BLACK   \n",
       "133265    19712154  34421659      M           WALK IN        HOME  WHITE   \n",
       "92830     16280495  34330125      M         AMBULANCE        HOME  WHITE   \n",
       "357427    19944585  35033215      F         AMBULANCE    ADMITTED  WHITE   \n",
       "316567    15777013  30740732      M           WALK IN    ADMITTED  WHITE   \n",
       "\n",
       "        age_at_ed       dow  hour  minute  temperature  heartrate  resprate  \\\n",
       "155086         51  Saturday     8      17         98.8      100.0      19.0   \n",
       "133265         55    Friday    12       6         96.5       65.0      18.0   \n",
       "92830          49  Thursday     9      46         98.0      100.0      18.0   \n",
       "357427         85    Friday    12      18         99.6       65.0      20.0   \n",
       "316567         70   Tuesday    18      10         97.8       58.0      16.0   \n",
       "\n",
       "        o2sat    sbp   dbp  pain  acuity  \n",
       "155086   99.0  182.0  77.0    10       2  \n",
       "133265  100.0  141.0  98.0     5       3  \n",
       "92830    98.0  114.0  84.0     0       2  \n",
       "357427   95.0  141.0  79.0     6       3  \n",
       "316567  100.0  168.0  86.0     3       3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy import stats, ndimage, interpolate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import google.auth\n",
    "warnings.filterwarnings('ignore')  # Suppresses all warnings\n",
    "\n",
    "# Add parent directory to Python path for local imports\n",
    "notebook_path = Path.cwd()  # Gets current working directory\n",
    "project_root = notebook_path.parent.parent  # Navigate up to project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Local application imports\n",
    "from src.mimicdf import MIMICDF\n",
    "from src.preprocessing.data_preprocessor import DataCleaner\n",
    "\n",
    "# Initialize MIMIC database connection to GCP\n",
    "mimicdf = MIMICDF.create_connection()\n",
    "\n",
    "# Initialize MIMIC demo database\n",
    "# mimicdf = MIMICDF.create_demo()\n",
    "\n",
    "# Help\n",
    "# printhelp(mimicdf)\n",
    "\n",
    "ed_data = mimicdf.ed_data()\n",
    "ed_data.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic Statistics\n",
    "ed_data.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Overall missingness by column\n",
    "\n",
    "def initial_missingness_analysis(data):\n",
    "    \"\"\"\n",
    "    Perform initial analysis of missingness patterns in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing_Count': data.isnull().sum(),\n",
    "        'Missing_Percentage': (data.isnull().sum() / len(data) * 100).round(2)\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    # 2. Visualize missingness patterns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=missing_summary.index, \n",
    "                y='Missing_Percentage',\n",
    "                data=missing_summary)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Missing Data Percentage by Variable')\n",
    "    plt.ylabel('Missing Percentage')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "# Run initial analysis\n",
    "missing_summary = initial_missingness_analysis(ed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Correlation heatmap only for features with missing values\n",
    "\n",
    "def analyze_missingness(data):\n",
    "\n",
    "    missing_features = ['age_at_ed', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "    features_with_missing = missing_features\n",
    "    \n",
    "    # Create binary missingness indicators only for features with missing values\n",
    "    missing_binary = data[features_with_missing].isnull().astype(int)\n",
    "    \n",
    "    # Compute correlation between missing indicators\n",
    "    missing_corr = missing_binary.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(missing_corr, \n",
    "                annot=True, \n",
    "                cmap='YlOrRd',\n",
    "                fmt='.2f',\n",
    "                center=0)\n",
    "    plt.title('Correlation of Missingness Patterns\\n(Features with Missing Values Only)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return missing_corr\n",
    "\n",
    "missing_corr = analyze_missingness(ed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Strong Correlations Among Vitals (0.75-0.98):\n",
    "    - Temperature, heartrate, resprate, o2sat, sbp, and dbp show very high correlations (dark red)\n",
    "    - This suggests these vitals tend to be missing together\n",
    "    - Makes sense clinically as they're usually measured in the same assessment\n",
    "2. Age Independence (-0.02 to -0.07):\n",
    "    - Age missingness shows very weak negative correlations with vital signs\n",
    "    - Suggets that age must be handled separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_counts(data):\n",
    "    \"\"\"\n",
    "    Prepares data for missing counts analysis\n",
    "    \"\"\"\n",
    "    # Define vital signs\n",
    "    vital_signs = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp']\n",
    "    \n",
    "    # Create a copy of data to avoid modifications to original\n",
    "    _df = data.copy()\n",
    "    \n",
    "    # Create missing indicators\n",
    "    _df['missing_vitals'] = _df[vital_signs].isnull().any(axis=1).astype(int)\n",
    "    _df['missing_age'] = _df['age_at_ed'].isnull().astype(int)\n",
    "    _df['missing_pain'] = _df['pain'].isnull().astype(int)\n",
    "    _df['missing_acuity'] = _df['acuity'].isnull().astype(int)\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    _df = _df[['missing_vitals', 'missing_age', 'missing_pain', 'missing_acuity', 'arrival_transport', 'disposition']].reset_index(drop=True)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def missing_counts_graph_annotated(data):\n",
    "    \"\"\"\n",
    "    Creates a 2x3 grid of charts analyzing missing data patterns\n",
    "    with annotations of missingness ratios.\n",
    "    \"\"\"\n",
    "    # Process the data (assuming 'missing_counts' is a function you have defined)\n",
    "    _df = missing_counts(data)\n",
    "\n",
    "    # Create figure with more space for titles\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Create GridSpec to allow for row labels and column labels\n",
    "    gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 0.1], width_ratios=[0.1, 1, 1, 1])\n",
    "    \n",
    "    # Create main subplot axes (2 rows, 3 columns)\n",
    "    axes = np.array([\n",
    "        [fig.add_subplot(gs[0, 1]), fig.add_subplot(gs[0, 2]), fig.add_subplot(gs[0, 3])],\n",
    "        [fig.add_subplot(gs[1, 1]), fig.add_subplot(gs[1, 2]), fig.add_subplot(gs[1, 3])],\n",
    "    ])\n",
    "\n",
    "    # Remove spines and format all subplots\n",
    "    for ax in axes.flat:\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        # Format x-axis in thousands\n",
    "        ax.xaxis.set_major_formatter(lambda x, p: f'{int(x/1000)}')\n",
    "        ax.grid(True, axis='x', alpha=0.75)\n",
    "        # Adjust as needed\n",
    "        ax.set_xlim(0, 300000)\n",
    "\n",
    "    # Remove x-axis labels and spines for the top row\n",
    "    axes[0,0].xaxis.set_ticklabels([])\n",
    "    axes[0,1].xaxis.set_ticklabels([])\n",
    "    axes[0,2].xaxis.set_ticklabels([])\n",
    "    axes[0,0].spines['bottom'].set_visible(False)\n",
    "    axes[0,1].spines['bottom'].set_visible(False)\n",
    "    axes[0,2].spines['bottom'].set_visible(False)\n",
    "\n",
    "    # Remove y-axis labels and spine for the right side\n",
    "    axes[0,1].yaxis.set_visible(False)\n",
    "    axes[0,1].spines['left'].set_visible(False)\n",
    "    axes[0,2].yaxis.set_visible(False)\n",
    "    axes[0,2].spines['left'].set_visible(False)\n",
    "    axes[1,1].yaxis.set_visible(False)\n",
    "    axes[1,1].spines['left'].set_visible(False)\n",
    "    axes[1,2].yaxis.set_visible(False)\n",
    "    axes[1,2].spines['left'].set_visible(False)\n",
    "\n",
    "    # Add 'Count' label for bottom row\n",
    "    axes[1,0].set_xlabel('Count (Thousands)')\n",
    "    axes[1,1].set_xlabel('Count (Thousands)')\n",
    "    axes[1,2].set_xlabel('Count (Thousands)')\n",
    "\n",
    "    # Add row labels (rotated 90 degrees)\n",
    "    row_labels = ['Arrival', 'Disposition']\n",
    "    for idx, label in enumerate(row_labels):\n",
    "        fig.add_subplot(gs[idx, 0]).text(0.5, 0.5, label, \n",
    "                                         rotation=90, \n",
    "                                         ha='center', \n",
    "                                         va='center',\n",
    "                                         fontsize=18,\n",
    "                                         fontweight='bold')\n",
    "        plt.axis('off')  # Hide the axis for the label subplots\n",
    "\n",
    "    # Add column labels at the bottom\n",
    "    col_labels = ['Missing Vitals', 'Missing Acuity', 'Missing Pain']\n",
    "    for idx, label in enumerate(col_labels):\n",
    "        fig.add_subplot(gs[2, idx+1]).text(0.5, 0.5, label,\n",
    "                                           ha='center',\n",
    "                                           va='center',\n",
    "                                           fontsize=18,\n",
    "                                           fontweight='bold')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Add title and subtitle\n",
    "    fig.suptitle('Missing Data Analysis in Emergency Department Records', \n",
    "                 y=0.95, fontsize=22, weight='bold')\n",
    "    plt.figtext(0.5, 0.91, \n",
    "                'Distribution of missing vital signs, age, and pain data across arrival modes and dispositions',\n",
    "                ha='center', fontsize=18)\n",
    "\n",
    "    # Helper function to convert to camel case (for y-tick labels)\n",
    "    def to_camel_case(text):\n",
    "        words = text.replace('_', ' ').title().split()\n",
    "        camel_case = words[0] + ' ' + ' '.join(word.capitalize() for word in words[1:])\n",
    "        return camel_case if len(camel_case) <= 15 else camel_case[:11] + '...'\n",
    "\n",
    "    # 1) Missing Vitals by arrival_transport (top-left)\n",
    "    transport_counts = _df.groupby('arrival_transport').size()\n",
    "    missing_vitals_counts = _df.groupby('arrival_transport')['missing_vitals'].sum()\n",
    "    sort_idx = transport_counts.sort_values(ascending=True).index\n",
    "    transport_counts = transport_counts[sort_idx]\n",
    "    missing_vitals_counts = missing_vitals_counts[sort_idx]\n",
    "    \n",
    "    ax = axes[0,0]\n",
    "    y_pos = np.arange(len(transport_counts))\n",
    "\n",
    "    ax.barh(y_pos, transport_counts, color='lightgray', alpha=0.5, label='Total Count')\n",
    "    bars_vitals = ax.barh(y_pos, missing_vitals_counts, color='skyblue', alpha=0.7, label='Missing Vitals')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([to_camel_case(str(x)) for x in sort_idx])\n",
    "\n",
    "    # Annotate missing_vitals bars with ratio (no decimals, e.g. 25%)\n",
    "    for i, bar in enumerate(bars_vitals):\n",
    "        total_val = transport_counts.iloc[i]\n",
    "        missing_val = missing_vitals_counts.iloc[i]\n",
    "        if total_val > 0:\n",
    "            ratio = int((missing_val / total_val) * 100)  # no decimals\n",
    "            x_bar = bar.get_width()\n",
    "            y_bar = bar.get_y() + bar.get_height() / 2\n",
    "            # Place text just to the right of the bar\n",
    "            ax.text(x_bar + 5000,\n",
    "                    y_bar,\n",
    "                    f\"{ratio}%\",\n",
    "                    va='center', ha='left',\n",
    "                    color='black',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')\n",
    "\n",
    "    # 2) Missing Vitals by disposition (bottom-left)\n",
    "    disposition_counts = _df.groupby('disposition').size()\n",
    "    missing_vitals_counts = _df.groupby('disposition')['missing_vitals'].sum()\n",
    "    sort_idx = disposition_counts.sort_values(ascending=True).index\n",
    "    disposition_counts = disposition_counts[sort_idx]\n",
    "    missing_vitals_counts = missing_vitals_counts[sort_idx]\n",
    "    \n",
    "    ax = axes[1,0]\n",
    "    y_pos = np.arange(len(disposition_counts))\n",
    "\n",
    "    ax.barh(y_pos, disposition_counts, color='lightgray', alpha=0.5, label='Total Count')\n",
    "    bars_vitals = ax.barh(y_pos, missing_vitals_counts, color='skyblue', alpha=0.7, label='Missing Vitals')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([to_camel_case(str(x)) for x in sort_idx])\n",
    "\n",
    "    # Annotate missing_vitals bars with ratio\n",
    "    for i, bar in enumerate(bars_vitals):\n",
    "        total_val = disposition_counts.iloc[i]\n",
    "        missing_val = missing_vitals_counts.iloc[i]\n",
    "        if total_val > 0:\n",
    "            ratio = int((missing_val / total_val) * 100)\n",
    "            x_bar = bar.get_width()\n",
    "            y_bar = bar.get_y() + bar.get_height() / 2\n",
    "            ax.text(x_bar + 5000,\n",
    "                    y_bar,\n",
    "                    f\"{ratio}%\",\n",
    "                    va='center', ha='left',\n",
    "                    color='black',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')\n",
    "\n",
    "    # 3) Missing Acuity by arrival_transport (top-middle)\n",
    "    arrival_transport_counts = _df.groupby('arrival_transport').size()\n",
    "    missing_acuity_counts = _df.groupby('arrival_transport')['missing_acuity'].sum()\n",
    "    sort_idx = arrival_transport_counts.sort_values(ascending=True).index\n",
    "    arrival_transport_counts = arrival_transport_counts[sort_idx]\n",
    "    missing_acuity_counts = missing_acuity_counts[sort_idx]\n",
    "    \n",
    "    ax = axes[0,1]\n",
    "    y_pos = np.arange(len(missing_acuity_counts))\n",
    "\n",
    "    ax.barh(y_pos, arrival_transport_counts, color='lightgray', alpha=0.5, label='Total Count')\n",
    "    bars_acuity = ax.barh(y_pos, missing_acuity_counts, color='lightcoral', alpha=0.7, label='Missing Acuity')\n",
    "\n",
    "    # Annotate missing_acuity bars with ratio\n",
    "    for i, bar in enumerate(bars_acuity):\n",
    "        total_val = arrival_transport_counts.iloc[i]\n",
    "        missing_val = missing_acuity_counts.iloc[i]\n",
    "        if total_val > 0:\n",
    "            ratio = int((missing_val / total_val) * 100)\n",
    "            x_bar = bar.get_width()\n",
    "            y_bar = bar.get_y() + bar.get_height() / 2\n",
    "            ax.text(x_bar + 5000,\n",
    "                    y_bar,\n",
    "                    f\"{ratio}%\",\n",
    "                    va='center', ha='left',\n",
    "                    color='black',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')  \n",
    "\n",
    "    # 4) Missing Acuity by disposition (bottom-middle)\n",
    "    disposition_counts = _df.groupby('disposition').size()\n",
    "    missing_acuity_counts = _df.groupby('disposition')['missing_acuity'].sum()\n",
    "    sort_idx = disposition_counts.sort_values(ascending=True).index\n",
    "    disposition_counts = disposition_counts[sort_idx]\n",
    "    missing_acuity_counts = missing_acuity_counts[sort_idx]\n",
    "\n",
    "    ax = axes[1,1]\n",
    "    y_pos = np.arange(len(missing_acuity_counts))\n",
    "\n",
    "    ax.barh(y_pos, disposition_counts, color='lightgray', alpha=0.5, label='Total Count')\n",
    "    bars_acuity = ax.barh(y_pos, missing_acuity_counts, color='lightcoral', alpha=0.7, label='Missing Acuity')\n",
    "    \n",
    "    # Annotate missing_acuity bars with ratio\n",
    "    for i, bar in enumerate(bars_acuity):\n",
    "        total_val = disposition_counts.iloc[i]\n",
    "        missing_val = missing_acuity_counts.iloc[i]\n",
    "        if total_val > 0:\n",
    "            ratio = int((missing_val / total_val) * 100)\n",
    "            x_bar = bar.get_width()\n",
    "            y_bar = bar.get_y() + bar.get_height() / 2\n",
    "            ax.text(x_bar + 5000,\n",
    "                    y_bar,\n",
    "                    f\"{ratio}%\",\n",
    "                    va='center', ha='left',\n",
    "                    color='black',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')\n",
    "\n",
    "    # 5) Missing Pain by arrival_transport (top-right)\n",
    "    arrival_transport_counts = _df.groupby('arrival_transport').size()\n",
    "    missing_pain_counts = _df.groupby('arrival_transport')['missing_pain'].sum()\n",
    "    sort_idx = arrival_transport_counts.sort_values(ascending=True).index\n",
    "    arrival_transport_counts = arrival_transport_counts[sort_idx]\n",
    "    missing_pain_counts = missing_pain_counts[sort_idx]\n",
    "    \n",
    "    ax = axes[0,2]\n",
    "    y_pos = np.arange(len(missing_pain_counts))\n",
    "\n",
    "    ax.barh(y_pos, arrival_transport_counts, color='lightgray', alpha=0.5, label='Total Count')\n",
    "    bars_pain = ax.barh(y_pos, missing_pain_counts, color='plum', alpha=0.7, label='Missing Pain')\n",
    "    \n",
    "    # Annotate missing_pain bars with ratio\n",
    "    for i, bar in enumerate(bars_pain):\n",
    "        total_val = arrival_transport_counts.iloc[i]\n",
    "        missing_val = missing_pain_counts.iloc[i]\n",
    "        if total_val > 0:\n",
    "            ratio = int((missing_val / total_val) * 100)\n",
    "            x_bar = bar.get_width()\n",
    "            y_bar = bar.get_y() + bar.get_height() / 2\n",
    "            ax.text(x_bar + 5000,\n",
    "                    y_bar,\n",
    "                    f\"{ratio}%\",\n",
    "                    va='center', ha='left',\n",
    "                    color='black',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')\n",
    "\n",
    "    # 6) Missing Pain by disposition (bottom-right)\n",
    "    disposition_counts = _df.groupby('disposition').size()\n",
    "    missing_pain_counts = _df.groupby('disposition')['missing_pain'].sum()\n",
    "    sort_idx = disposition_counts.sort_values(ascending=True).index\n",
    "    disposition_counts = disposition_counts[sort_idx]\n",
    "    missing_pain_counts = missing_pain_counts[sort_idx]\n",
    "    \n",
    "    ax = axes[1,2]\n",
    "    y_pos = np.arange(len(missing_pain_counts))\n",
    "\n",
    "    ax.barh(y_pos, disposition_counts, color='lightgray', alpha=0.5, label='Total Count')\n",
    "    bars_pain = ax.barh(y_pos, missing_pain_counts, color='plum', alpha=0.7, label='Missing Pain')\n",
    "    \n",
    "    # Annotate missing_pain bars with ratio\n",
    "    for i, bar in enumerate(bars_pain):\n",
    "        total_val = disposition_counts.iloc[i]\n",
    "        missing_val = missing_pain_counts.iloc[i]\n",
    "        if total_val > 0:\n",
    "            ratio = int((missing_val / total_val) * 100)\n",
    "            x_bar = bar.get_width()\n",
    "            y_bar = bar.get_y() + bar.get_height() / 2\n",
    "            ax.text(x_bar + 5000,\n",
    "                    y_bar,\n",
    "                    f\"{ratio}%\",\n",
    "                    va='center', ha='left',\n",
    "                    color='black',\n",
    "                    fontsize=11,\n",
    "                    fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Adjust layout to make room for the main title\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "missing_counts_graph_annotated(ed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_distribution(ed_data):\n",
    "    plt.style.use('default')  # Using default style instead of seaborn\n",
    "    features = ['age_at_ed', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + 2) // 3  # Calculate number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n",
    "    axes = axes.ravel()  # Flatten axes array for easier indexing\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        # Create boxplot and histogram side by side\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create violin plot with boxplot inside\n",
    "        sns.violinplot(data=ed_data, y=feature, ax=ax, inner='box', color='lightblue')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title(f'{feature} Distribution')\n",
    "        ax.set_ylabel(feature)\n",
    "        \n",
    "        # Add text with basic statistics\n",
    "        stats_text = f'Mean: {ed_data[feature].mean():.2f}\\n'\n",
    "        stats_text += f'Median: {ed_data[feature].median():.2f}\\n'\n",
    "        stats_text += f'Missing: {ed_data[feature].isna().sum()/len(ed_data)*100:.1f}%'\n",
    "        ax.text(0.95, 0.95, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Remove any empty subplots\n",
    "    for idx in range(n_features, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with ed_data\n",
    "# plot_features_distribution(ed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataCleaner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     36\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Call the function with ed_data\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# plot_features_distribution(ed_data)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Preprocess ed_data to handle invlaid values\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m data_preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mDataCleaner\u001b[49m(ed_data)\n\u001b[1;32m     44\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m data_preprocessor\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[1;32m     46\u001b[0m plot_features_distribution(cleaned_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataCleaner' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_features_distribution(ed_data):\n",
    "    plt.style.use('default')  # Using default style instead of seaborn\n",
    "    features = ['age_at_ed', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "    n_features = len(features)\n",
    "    n_rows = (n_features + 2) // 3  # Calculate number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n",
    "    axes = axes.ravel()  # Flatten axes array for easier indexing\n",
    "\n",
    "    for idx, feature in enumerate(features):\n",
    "        # Create boxplot and histogram side by side\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create violin plot with boxplot inside\n",
    "        sns.violinplot(data=ed_data, y=feature, ax=ax, inner='box', color='lightblue')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title(f'{feature} Distribution')\n",
    "        ax.set_ylabel(feature)\n",
    "        \n",
    "        # Add text with basic statistics\n",
    "        stats_text = f'Mean: {ed_data[feature].mean():.2f}\\n'\n",
    "        stats_text += f'Median: {ed_data[feature].median():.2f}\\n'\n",
    "        stats_text += f'Missing: {ed_data[feature].isna().sum()/len(ed_data)*100:.1f}%'\n",
    "        ax.text(0.95, 0.95, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                verticalalignment='top',\n",
    "                horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Remove any empty subplots\n",
    "    for idx in range(n_features, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with ed_data\n",
    "# plot_features_distribution(ed_data)\n",
    "\n",
    "\n",
    "# Preprocess ed_data to handle invlaid values\n",
    "data_preprocessor = DataCleaner(ed_data)\n",
    "cleaned_data = data_preprocessor.prepare_data()\n",
    "\n",
    "plot_features_distribution(cleaned_data)\n",
    "cleaned_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution plot\n",
    "sns.histplot(data=cleaned_data, x='los_minutes', bins=50, ax=ax1)\n",
    "ax1.set_title('Distribution of Length of Stay')\n",
    "ax1.set_xlabel('Length of Stay (minutes)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Add mean and median lines\n",
    "mean_los = cleaned_data['los_minutes'].mean()\n",
    "median_los = cleaned_data['los_minutes'].median()\n",
    "ax1.axvline(mean_los, color='red', linestyle='--', label=f'Mean: {mean_los:.1f}')\n",
    "ax1.axvline(median_los, color='green', linestyle='--', label=f'Median: {median_los:.1f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(cleaned_data['los_minutes'], dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot of Length of Stay')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics for Length of Stay (minutes):\")\n",
    "print(cleaned_data['los_minutes'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR-based bounds\n",
    "Q1 = cleaned_data['los_minutes'].quantile(0.25)\n",
    "Q3 = cleaned_data['los_minutes'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Set lower and upper bounds\n",
    "# Lower bound: 0 minutes (can't have negative LOS)\n",
    "# Upper bound: Q3 + 1.5*IQR (standard outlier definition)\n",
    "lower_bound = 0\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter data\n",
    "filtered_data = cleaned_data[\n",
    "    (cleaned_data['los_minutes'] >= lower_bound) & \n",
    "    (cleaned_data['los_minutes'] <= upper_bound)\n",
    "]\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Original data points: {len(cleaned_data)}\")\n",
    "print(f\"Filtered data points: {len(filtered_data)}\")\n",
    "print(f\"Removed {len(cleaned_data) - len(filtered_data)} outliers\")\n",
    "print(f\"\\nUpper bound: {upper_bound:.1f} minutes ({upper_bound/60:.1f} hours)\")\n",
    "\n",
    "# Plot new distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution plot\n",
    "sns.histplot(data=filtered_data, x='los_minutes', bins=50, ax=ax1)\n",
    "ax1.set_title('Distribution of Length of Stay (Filtered)')\n",
    "ax1.set_xlabel('Length of Stay (minutes)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Add mean and median lines\n",
    "mean_los = filtered_data['los_minutes'].mean()\n",
    "median_los = filtered_data['los_minutes'].median()\n",
    "ax1.axvline(mean_los, color='red', linestyle='--', label=f'Mean: {mean_los:.1f}')\n",
    "ax1.axvline(median_los, color='green', linestyle='--', label=f'Median: {median_los:.1f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(filtered_data['los_minutes'], dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot of Length of Stay (Filtered)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate both IQR and STD bounds\n",
    "mean = cleaned_data['los_minutes'].mean()\n",
    "std = cleaned_data['los_minutes'].std()\n",
    "\n",
    "# Set bounds\n",
    "# STD method: mean Â± 3 standard deviations (99.7% of data in normal distribution)\n",
    "std_lower = mean - 3 * std\n",
    "std_upper = mean + 3 * std\n",
    "# Ensure lower bound isn't negative\n",
    "std_lower = max(0, std_lower)\n",
    "\n",
    "# Filter data using STD method\n",
    "std_filtered_data = cleaned_data[\n",
    "    (cleaned_data['los_minutes'] >= std_lower) & \n",
    "    (cleaned_data['los_minutes'] <= std_upper)\n",
    "]\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution plot\n",
    "sns.histplot(data=std_filtered_data, x='los_minutes', bins=50, ax=ax1)\n",
    "ax1.set_title('Distribution of Length of Stay (STD Filtered)')\n",
    "ax1.set_xlabel('Length of Stay (minutes)')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Add mean and median lines\n",
    "mean_los = std_filtered_data['los_minutes'].mean()\n",
    "median_los = std_filtered_data['los_minutes'].median()\n",
    "ax1.axvline(mean_los, color='red', linestyle='--', label=f'Mean: {mean_los:.1f}')\n",
    "ax1.axvline(median_los, color='green', linestyle='--', label=f'Median: {median_los:.1f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(std_filtered_data['los_minutes'], dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot of Length of Stay (STD Filtered)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Original data points: {len(cleaned_data)}\")\n",
    "print(f\"STD filtered data points: {len(std_filtered_data)}\")\n",
    "print(f\"Removed {len(cleaned_data) - len(std_filtered_data)} outliers\")\n",
    "print(f\"\\nUpper bound: {std_upper:.1f} minutes ({std_upper/60:.1f} hours)\")\n",
    "print(f\"Lower bound: {std_lower:.1f} minutes ({std_lower/60:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = cleaned_data.sample(100)\n",
    "# sns pairplot of all columns in cleaned_data with kde, hue = disposition\n",
    "sns.pairplot(sample_data, hue='disposition', kind='kde')\n",
    "plt.show()\n",
    "\n",
    "# sns pairplot of all columns in cleaned_data with kde, hue = arrival_transport\n",
    "sns.pairplot(sample_data, hue='arrival_transport', kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Select numerical features for clustering\n",
    "numerical_features = ['age_at_ed', 'temperature', 'heartrate', 'resprate', \n",
    "                     'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "\n",
    "# Prepare the data\n",
    "X = cleaned_data[numerical_features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize and fit GMM\n",
    "n_components = 5  # You can adjust this number\n",
    "gmm = GaussianMixture(n_components=n_components, \n",
    "                      random_state=42,\n",
    "                      covariance_type='full',\n",
    "                      n_init=10)\n",
    "gmm.fit(X_scaled)\n",
    "\n",
    "# Get cluster assignments\n",
    "clusters = gmm.predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "cleaned_data['cluster'] = clusters\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "cluster_stats = cleaned_data.groupby('cluster')[numerical_features].agg(['mean', 'std'])\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Cluster sizes\n",
    "plt.subplot(1, 2, 1)\n",
    "cluster_sizes = cleaned_data['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_sizes.index, cluster_sizes.values)\n",
    "plt.title('Cluster Sizes')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Samples')\n",
    "\n",
    "# Plot 2: BIC scores for different numbers of components\n",
    "plt.subplot(1, 2, 2)\n",
    "n_components_range = range(1, 11)\n",
    "bic = []\n",
    "for n in n_components_range:\n",
    "    gmm_temp = GaussianMixture(n_components=n, random_state=42, n_init=10)\n",
    "    gmm_temp.fit(X_scaled)\n",
    "    bic.append(gmm_temp.bic(X_scaled))\n",
    "    \n",
    "plt.plot(n_components_range, bic, marker='o')\n",
    "plt.title('BIC Score vs. Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster statistics\n",
    "print(\"\\nCluster Statistics:\")\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_circles_from_stats(cluster_stats, feature_names):\n",
    "    # Create pairs of features\n",
    "    feature_pairs = list(itertools.combinations(range(len(feature_names)), 2))\n",
    "    \n",
    "    # Calculate number of rows and columns needed\n",
    "    n_pairs = len(feature_pairs)\n",
    "    n_cols = 3  # You can adjust this\n",
    "    n_rows = (n_pairs + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Colors for clusters\n",
    "    n_clusters = len(cluster_stats.index)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    # For each pair of features\n",
    "    for idx, (i, j) in enumerate(feature_pairs):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        feature_i = feature_names[i]\n",
    "        feature_j = feature_names[j]\n",
    "        \n",
    "        # Plot circles for each cluster\n",
    "        for k in range(n_clusters):\n",
    "            # Get mean and std for these features\n",
    "            mean_i = cluster_stats.iloc[k][f'{feature_i}']['mean']\n",
    "            mean_j = cluster_stats.iloc[k][f'{feature_j}']['mean']\n",
    "            std_i = cluster_stats.iloc[k][f'{feature_i}']['std']\n",
    "            std_j = cluster_stats.iloc[k][f'{feature_j}']['std']\n",
    "            \n",
    "            # Create circle\n",
    "            circle = patches.Ellipse((mean_i, mean_j), \n",
    "                                   width=2*std_i, \n",
    "                                   height=2*std_j,\n",
    "                                   alpha=0.3,\n",
    "                                   color=colors[k],\n",
    "                                   label=f'Cluster {k}')\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Add cluster center\n",
    "            ax.plot(mean_i, mean_j, 'o', \n",
    "                   color=colors[k], \n",
    "                   markersize=10,\n",
    "                   markeredgecolor='black')\n",
    "        \n",
    "        # Set labels\n",
    "        ax.set_xlabel(feature_i)\n",
    "        ax.set_ylabel(feature_j)\n",
    "        \n",
    "        # Add legend to first plot only\n",
    "        if idx == 0:\n",
    "            ax.legend()\n",
    "            \n",
    "        # Set title\n",
    "        ax.set_title(f'{feature_i} vs {feature_j}')\n",
    "        \n",
    "        # Make plot square\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(len(feature_pairs), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define feature names\n",
    "feature_names = ['age_at_ed', 'temperature', 'heartrate', 'resprate', \n",
    "                'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "\n",
    "# Create visualization\n",
    "plot_cluster_circles_from_stats(cluster_stats, feature_names)\n",
    "\n",
    "# You might also want to see just a few key relationships\n",
    "key_features = ['age_at_ed', 'heartrate', 'resprate', 'pain']\n",
    "plot_cluster_circles_from_stats(cluster_stats, key_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Select numerical features for clustering\n",
    "numerical_features = ['age_at_ed', 'temperature', 'heartrate', 'resprate', \n",
    "                     'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "\n",
    "# Sample data and prepare for clustering\n",
    "sample_size = 10000\n",
    "sampled_data = cleaned_data.sample(n=sample_size, random_state=42)\n",
    "X = sampled_data[numerical_features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize and fit HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=50,  # Minimum size of clusters\n",
    "    min_samples=5,        # Number of samples in neighborhood for core points\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'  # 'eom' tends to produce more clusters than 'leaf'\n",
    ")\n",
    "clusters = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the sampled dataframe\n",
    "sampled_data['cluster'] = clusters\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot cluster sizes\n",
    "cluster_sizes = sampled_data['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_sizes.index, cluster_sizes.values)\n",
    "plt.title('Cluster Sizes')\n",
    "plt.xlabel('Cluster (-1 represents noise points)')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster statistics\n",
    "print(\"\\nCluster Statistics:\")\n",
    "cluster_stats = sampled_data.groupby('cluster')[numerical_features].agg(['mean', 'std'])\n",
    "print(cluster_stats)\n",
    "\n",
    "# Optional: Plot feature distributions by cluster\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cluster in sorted(sampled_data['cluster'].unique()):\n",
    "        cluster_data = sampled_data[sampled_data['cluster'] == cluster][feature]\n",
    "        plt.hist(cluster_data, alpha=0.5, label=f'Cluster {cluster}', bins=30)\n",
    "    plt.title(f'{feature} Distribution by Cluster')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Select numerical features for clustering - removing ordinal variables\n",
    "numerical_features = ['age_at_ed', 'temperature', 'heartrate', 'resprate', \n",
    "                     'o2sat', 'sbp', 'dbp']\n",
    "\n",
    "# Sample data and prepare for clustering\n",
    "sample_size = 10000\n",
    "sampled_data = cleaned_data.sample(n=sample_size, random_state=42)\n",
    "X = sampled_data[numerical_features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Try different HDBSCAN configurations\n",
    "configs = [\n",
    "    {'min_cluster_size': 100, 'min_samples': 5},\n",
    "    {'min_cluster_size': 200, 'min_samples': 10},\n",
    "    {'min_cluster_size': 50, 'min_samples': 2},\n",
    "    {'min_cluster_size': 30, 'min_samples': 1}\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    # Initialize and fit HDBSCAN with current config\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=config['min_cluster_size'],\n",
    "        min_samples=config['min_samples'],\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        cluster_selection_epsilon=0.5  # More permissive cluster selection\n",
    "    )\n",
    "    clusters = clusterer.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate noise percentage\n",
    "    noise_percentage = (clusters == -1).mean() * 100\n",
    "    \n",
    "    # Plot cluster sizes\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    cluster_sizes = pd.Series(clusters).value_counts().sort_index()\n",
    "    plt.bar(cluster_sizes.index, cluster_sizes.values)\n",
    "    plt.title(f'Cluster Sizes\\nConfig: {config}\\nNoise: {noise_percentage:.1f}%')\n",
    "    plt.xlabel('Cluster (-1 represents noise points)')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if noise_percentage < 50:  # Only show detailed stats if we have reasonable clustering\n",
    "        # Add cluster labels to a copy of sampled data\n",
    "        temp_data = sampled_data.copy()\n",
    "        temp_data['cluster'] = clusters\n",
    "        \n",
    "        # Print cluster statistics\n",
    "        print(f\"\\nCluster Statistics for config {config}:\")\n",
    "        cluster_stats = temp_data.groupby('cluster')[numerical_features].agg(['mean', 'std'])\n",
    "        print(cluster_stats)\n",
    "        \n",
    "        # Plot feature distributions for non-noise clusters\n",
    "        for feature in numerical_features:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for cluster in sorted(set(clusters[clusters != -1])):  # Skip noise cluster\n",
    "                cluster_data = temp_data[temp_data['cluster'] == cluster][feature]\n",
    "                plt.hist(cluster_data, alpha=0.5, label=f'Cluster {cluster}', bins=30)\n",
    "            plt.title(f'{feature} Distribution by Cluster\\nConfig: {config}')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Select numerical features for clustering\n",
    "numerical_features = ['age_at_ed', 'temperature', 'heartrate', 'resprate', \n",
    "                     'o2sat', 'sbp', 'dbp']\n",
    "\n",
    "# Sample data and prepare for clustering\n",
    "sample_size = 10000\n",
    "sampled_data = cleaned_data.sample(n=sample_size, random_state=42)\n",
    "X = sampled_data[numerical_features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Try different numbers of clusters\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, clusters)\n",
    "    silhouette_scores.append(score)\n",
    "    \n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, silhouette_scores, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Use optimal k from silhouette analysis\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the sampled dataframe\n",
    "sampled_data['cluster'] = clusters\n",
    "\n",
    "# Plot cluster sizes\n",
    "plt.figure(figsize=(15, 6))\n",
    "cluster_sizes = sampled_data['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_sizes.index, cluster_sizes.values)\n",
    "plt.title(f'Cluster Sizes (k={optimal_k})')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n",
    "\n",
    "# Print cluster statistics\n",
    "print(\"\\nCluster Statistics:\")\n",
    "cluster_stats = sampled_data.groupby('cluster')[numerical_features].agg(['mean', 'std'])\n",
    "print(cluster_stats)\n",
    "\n",
    "# Plot feature distributions by cluster\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cluster in sorted(sampled_data['cluster'].unique()):\n",
    "        cluster_data = sampled_data[sampled_data['cluster'] == cluster][feature]\n",
    "        plt.hist(cluster_data, alpha=0.5, label=f'Cluster {cluster}', bins=30)\n",
    "    plt.title(f'{feature} Distribution by Cluster')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze how clusters relate to acuity\n",
    "print(\"\\nAcuity Distribution by Cluster:\")\n",
    "acuity_dist = pd.crosstab(sampled_data['cluster'], sampled_data['acuity'], normalize='index') * 100\n",
    "print(acuity_dist)\n",
    "\n",
    "# Visualize cluster centers\n",
    "cluster_centers = pd.DataFrame(\n",
    "    scaler.inverse_transform(kmeans.cluster_centers_),\n",
    "    columns=numerical_features\n",
    ")\n",
    "print(\"\\nCluster Centers (Original Scale):\")\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same sampled data and scaled features from previous analysis\n",
    "# Try different numbers of components and covariance types\n",
    "n_components_range = range(2, 11)\n",
    "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "# Initialize storage for BIC and AIC scores\n",
    "bic_scores = np.zeros((len(covariance_types), len(n_components_range)))\n",
    "aic_scores = np.zeros((len(covariance_types), len(n_components_range)))\n",
    "\n",
    "# Compute scores for each combination\n",
    "for i, cv_type in enumerate(covariance_types):\n",
    "    for j, n_comp in enumerate(n_components_range):\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=n_comp,\n",
    "            covariance_type=cv_type,\n",
    "            random_state=42,\n",
    "            n_init=10\n",
    "        )\n",
    "        gmm.fit(X_scaled)\n",
    "        bic_scores[i, j] = gmm.bic(X_scaled)\n",
    "        aic_scores[i, j] = gmm.aic(X_scaled)\n",
    "\n",
    "# Plot BIC and AIC scores\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# BIC plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, cv_type in enumerate(covariance_types):\n",
    "    plt.plot(n_components_range, bic_scores[i], label=cv_type, marker='o')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('BIC score')\n",
    "plt.title('BIC Score vs. Number of Components')\n",
    "plt.legend()\n",
    "\n",
    "# AIC plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, cv_type in enumerate(covariance_types):\n",
    "    plt.plot(n_components_range, aic_scores[i], label=cv_type, marker='o')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('AIC score')\n",
    "plt.title('AIC Score vs. Number of Components')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal parameters\n",
    "best_bic_idx = np.unravel_index(bic_scores.argmin(), bic_scores.shape)\n",
    "best_cv_type = covariance_types[best_bic_idx[0]]\n",
    "best_n_components = n_components_range[best_bic_idx[1]]\n",
    "\n",
    "print(f\"\\nOptimal parameters based on BIC:\")\n",
    "print(f\"Covariance type: {best_cv_type}\")\n",
    "print(f\"Number of components: {best_n_components}\")\n",
    "\n",
    "# Fit optimal GMM\n",
    "optimal_gmm = GaussianMixture(\n",
    "    n_components=best_n_components,\n",
    "    covariance_type=best_cv_type,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "clusters = optimal_gmm.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the sampled dataframe\n",
    "sampled_data['cluster'] = clusters\n",
    "\n",
    "# Plot cluster sizes\n",
    "plt.figure(figsize=(15, 6))\n",
    "cluster_sizes = sampled_data['cluster'].value_counts().sort_index()\n",
    "plt.bar(cluster_sizes.index, cluster_sizes.values)\n",
    "plt.title(f'GMM Cluster Sizes (n={best_n_components}, covariance={best_cv_type})')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.show()\n",
    "\n",
    "# Print cluster statistics\n",
    "print(\"\\nCluster Statistics:\")\n",
    "cluster_stats = sampled_data.groupby('cluster')[numerical_features].agg(['mean', 'std'])\n",
    "print(cluster_stats)\n",
    "\n",
    "# Plot feature distributions by cluster\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for cluster in sorted(sampled_data['cluster'].unique()):\n",
    "        cluster_data = sampled_data[sampled_data['cluster'] == cluster][feature]\n",
    "        plt.hist(cluster_data, alpha=0.5, label=f'Cluster {cluster}', bins=30)\n",
    "    plt.title(f'{feature} Distribution by Cluster')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze cluster probabilities\n",
    "cluster_probs = optimal_gmm.predict_proba(X_scaled)\n",
    "avg_prob = np.mean(np.max(cluster_probs, axis=1))\n",
    "print(f\"\\nAverage probability of cluster assignment: {avg_prob:.3f}\")\n",
    "\n",
    "# Analyze how clusters relate to acuity\n",
    "print(\"\\nAcuity Distribution by Cluster:\")\n",
    "acuity_dist = pd.crosstab(sampled_data['cluster'], sampled_data['acuity'], normalize='index') * 100\n",
    "print(acuity_dist)\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = pd.DataFrame(\n",
    "    scaler.inverse_transform(optimal_gmm.means_),\n",
    "    columns=numerical_features\n",
    ")\n",
    "print(\"\\nCluster Centers (Original Scale):\")\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimiced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
