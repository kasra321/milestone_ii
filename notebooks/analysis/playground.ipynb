{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MIMIC-IV ED dataset\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy import stats, ndimage, interpolate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import google.auth\n",
    "warnings.filterwarnings('ignore')  # Suppresses all warnings\n",
    "\n",
    "# Add parent directory to Python path for local imports\n",
    "notebook_path = Path.cwd()  # Gets current working directory\n",
    "project_root = notebook_path.parent.parent  # Navigate up to project root\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Local application imports\n",
    "from src.mimicdf import MIMICDF\n",
    "from src.preprocessing.data_preprocessor import DataPreprocessor\n",
    "\n",
    "# Initialize MIMIC database connection to GCP\n",
    "mimicdf = MIMICDF.create_connection()\n",
    "\n",
    "# Initialize MIMIC demo database\n",
    "# mimicdf = MIMICDF.create_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table loaded: edstays\n",
      "Table loaded: patients\n",
      "Table loaded: patients\n",
      "Loading edstays...\n",
      "Table loaded: edstays\n",
      "Loading demographics...\n",
      "Table loaded: edstays\n",
      "Table loaded: age\n",
      "Loading age data...\n",
      "Table loaded: patients\n",
      "Calculating ED visit age...\n",
      "Merging time features...\n",
      "Table loaded: edstays\n",
      "Merging triage features...\n",
      "Table loaded: triage\n",
      "Cleaning up columns...\n",
      "\n",
      " Dataframe shape: (425087, 18) \n",
      "\n",
      "Dataframe info: \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 425087 entries, 0 to 425086\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   subject_id         425087 non-null  Int64  \n",
      " 1   stay_id            425087 non-null  Int64  \n",
      " 2   gender             425087 non-null  object \n",
      " 3   arrival_transport  425087 non-null  object \n",
      " 4   disposition        425087 non-null  object \n",
      " 5   race               425087 non-null  object \n",
      " 6   age_at_ed          425087 non-null  Int64  \n",
      " 7   dow                425087 non-null  object \n",
      " 8   hour               425087 non-null  int32  \n",
      " 9   los_minutes        425087 non-null  float64\n",
      " 10  temperature        401672 non-null  Float64\n",
      " 11  heartrate          407997 non-null  Float64\n",
      " 12  resprate           404734 non-null  Float64\n",
      " 13  o2sat              404491 non-null  Float64\n",
      " 14  sbp                406796 non-null  Float64\n",
      " 15  dbp                405996 non-null  Float64\n",
      " 16  pain               396971 non-null  Int64  \n",
      " 17  acuity             418100 non-null  Int64  \n",
      "dtypes: Float64(6), Int64(5), float64(1), int32(1), object(5)\n",
      "memory usage: 61.2+ MB\n",
      "None\n",
      "Verifying no duplicate stay_ids...\n"
     ]
    }
   ],
   "source": [
    "edstays = mimicdf.edstays()\n",
    "patients = mimicdf.patients()\n",
    "age = mimicdf.age()\n",
    "ed_data = mimicdf.ed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425087, 18)\n",
      "(299712, 6)\n"
     ]
    }
   ],
   "source": [
    "print(ed_data.shape)\n",
    "print(age.shape)\n",
    "\n",
    "# Get unique subject_ids from both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique subjects in ED data: 205,504\n",
      "Total unique subjects in patients data: 299,712\n",
      "Subjects in both tables: 205,504\n",
      "Subjects only in ED data: 0\n",
      "Subjects only in patients data: 94,208\n",
      "\n",
      "Overlap percentage: 100.0% of ED subjects are in patients table\n"
     ]
    }
   ],
   "source": [
    "# Get unique subject_ids from both tables\n",
    "ed_subjects = set(edstays['subject_id'])\n",
    "patient_subjects = set(age['subject_id'])\n",
    "\n",
    "# Calculate overlaps\n",
    "total_ed = len(ed_subjects)\n",
    "total_patients = len(patient_subjects)\n",
    "common_subjects = len(ed_subjects.intersection(patient_subjects))\n",
    "only_in_ed = len(ed_subjects - patient_subjects)\n",
    "only_in_patients = len(patient_subjects - ed_subjects)\n",
    "\n",
    "print(f\"Total unique subjects in ED data: {total_ed:,}\")\n",
    "print(f\"Total unique subjects in patients data: {total_patients:,}\")\n",
    "print(f\"Subjects in both tables: {common_subjects:,}\")\n",
    "print(f\"Subjects only in ED data: {only_in_ed:,}\")\n",
    "print(f\"Subjects only in patients data: {only_in_patients:,}\")\n",
    "print(f\"\\nOverlap percentage: {(common_subjects/total_ed)*100:.1f}% of ED subjects are in patients table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject_id               0\n",
       "stay_id                  0\n",
       "gender                   0\n",
       "arrival_transport        0\n",
       "disposition              0\n",
       "race                     0\n",
       "age_at_ed                0\n",
       "dow                      0\n",
       "hour                     0\n",
       "los_minutes              0\n",
       "temperature          23415\n",
       "heartrate            17090\n",
       "resprate             20353\n",
       "o2sat                20596\n",
       "sbp                  18291\n",
       "dbp                  19091\n",
       "pain                 28116\n",
       "acuity                6987\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = DataPreprocessor(mimicdf)\n",
    "df = data_preprocessor.prepare_data()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingness_ratio = df.isna().mean()\n",
    "print('Missingness ratio:')\n",
    "missingness_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary indicators for missingness\n",
    "missing_matrix = df.isna().astype(int)\n",
    "\n",
    "# 1. Correlation of missingness\n",
    "missing_corr = missing_matrix.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(missing_corr, \n",
    "            cmap='RdBu_r', \n",
    "            center=0,\n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            square=True)\n",
    "plt.title('Correlation of Missing Values')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Analyze if missingness is related to observed values\n",
    "# For example, let's check if missing vital signs are related to age or acuity\n",
    "vital_signs = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp']\n",
    "observed_vars = ['age_at_ed', 'acuity']\n",
    "\n",
    "results = []\n",
    "for vital in vital_signs:\n",
    "    for var in observed_vars:\n",
    "        # Calculate mean of observed variable for missing and non-missing groups\n",
    "        missing_mean = df[df[vital].isna()][var].mean()\n",
    "        present_mean = df[~df[vital].isna()][var].mean()\n",
    "        \n",
    "        results.append({\n",
    "            'vital_sign': vital,\n",
    "            'observed_var': var,\n",
    "            'missing_mean': missing_mean,\n",
    "            'present_mean': present_mean,\n",
    "            'difference': missing_mean - present_mean\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of means between missing and non-missing groups:\")\n",
    "print(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on vital signs only\n",
    "vital_signs = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'pain']\n",
    "\n",
    "# Create correlation matrix of missingness for vital signs only\n",
    "vital_missing = df[vital_signs].isna().astype(int)\n",
    "vital_corr = vital_missing.corr()\n",
    "\n",
    "# Plot correlation heatmap with values for each cell\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = sns.heatmap(vital_corr, \n",
    "                      cmap='RdBu_r', \n",
    "                      center=0,\n",
    "                      square=True,\n",
    "                      linewidths=0.5)\n",
    "\n",
    "# Add annotations manually\n",
    "for i in range(len(vital_corr.index)):\n",
    "    for j in range(len(vital_corr.columns)):\n",
    "        value = vital_corr.iloc[i, j]\n",
    "        heatmap.text(j + 0.5, i + 0.5, f'{value:.2f}', \n",
    "                    ha='center', va='center',\n",
    "                    color='black', fontsize=10)\n",
    "\n",
    "plt.title('Correlation of Missing Values in Vital Signs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print missingness percentages\n",
    "missingness_pct = vital_missing.mean() * 100\n",
    "print(\"\\nPercentage of missing values for each vital sign:\")\n",
    "for vital, pct in missingness_pct.items():\n",
    "    print(f\"{vital}: {pct:.1f}%\")\n",
    "\n",
    "# print correlation matrix of vital signs only\n",
    "print('Missingness correlation matrix of vital signs:')\n",
    "print(vital_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows wtih missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# print missingness ratio of cleaned dataframe\n",
    "missingness_ratio_clean = df_clean.isna().mean()\n",
    "print('Missingness ratio of cleaned dataframe:')\n",
    "missingness_ratio_clean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of los_minutes\n",
    "plt.hist(df_clean['los_minutes'], bins=100)\n",
    "plt.show()\n",
    "\n",
    "# standardize los_minutes on a spearate dataframe\n",
    "df_los = df_clean['los_minutes'].copy()\n",
    "df_los = (df_los - df_los.mean()) / df_los.std()\n",
    "\n",
    "# histogram of los_minutes\n",
    "plt.hist(df_los, bins=100)\n",
    "plt.show()\n",
    "\n",
    "# standardize los_minutes with log transformation\n",
    "df_los = np.log(df_los)\n",
    "\n",
    "# histogram of los_minutes\n",
    "plt.hist(df_los, bins=100)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Option 1: Current log transformation (simple and interpretable)\n",
    "los_log = np.log(df_clean['los_minutes'])\n",
    "\n",
    "# Option 2: Box-Cox transformation (potentially better normalization)\n",
    "los_boxcox, lambda_param = stats.boxcox(df_clean['los_minutes'])\n",
    "\n",
    "# Compare distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Log transform\n",
    "ax1.hist(los_log, bins=100)\n",
    "ax1.set_title('Log transformed los_minutes')\n",
    "\n",
    "# Box-Cox\n",
    "ax2.hist(los_boxcox, bins=100)\n",
    "ax2.set_title('Box-Cox transformed los_minutes')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print Box-Cox lambda parameter\n",
    "print(f\"Box-Cox lambda parameter: {lambda_param:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograme of age_at_ed\n",
    "plt.hist(df_clean['age_at_ed'], bins=100)\n",
    "plt.title('Histogram of Age at ED')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# histograme of sbp\n",
    "plt.hist(df_clean['sbp'], bins=100)\n",
    "plt.title('Histogram of SBP')\n",
    "plt.xlabel('SBP')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# histogram of dbp\n",
    "plt.hist(df_clean['dbp'], bins=100)\n",
    "plt.title('Histogram of DBP')\n",
    "plt.xlabel('DBP')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# histogram of o2sat\n",
    "plt.hist(df_clean['o2sat'], bins=100)\n",
    "plt.title('Histogram of O2Sat')\n",
    "plt.xlabel('O2Sat')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# histogram of resprate\n",
    "plt.hist(df_clean['resprate'], bins=100)\n",
    "plt.title('Histogram of Resprate')\n",
    "plt.xlabel('Resprate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# histogram of temperature\n",
    "plt.hist(df_clean['temperature'], bins=100)\n",
    "plt.title('Histogram of Temperature')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# histogram of pain\n",
    "plt.hist(df_clean['pain'], bins=100)\n",
    "plt.title('Histogram of Pain')\n",
    "plt.xlabel('Pain')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Select features\n",
    "features = ['age_at_ed', 'los_minutes', 'heartrate', 'sbp', 'dbp', 'o2sat', \n",
    "           'resprate', 'temperature', 'pain']\n",
    "df_prep = df_clean[features].copy()\n",
    "\n",
    "# 2. Box-Cox transform los_minutes\n",
    "df_prep['los_minutes'], lambda_param = stats.boxcox(df_prep['los_minutes'])\n",
    "print(f\"Box-Cox lambda parameter for los_minutes: {lambda_param:.3f}\")\n",
    "\n",
    "# 3. Standard scale all other features\n",
    "other_features = [col for col in features if col != 'los_minutes']\n",
    "scaler = StandardScaler()\n",
    "df_prep[other_features] = scaler.fit_transform(df_prep[other_features])\n",
    "\n",
    "# 4. Create histograms\n",
    "n_features = len(features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "axes = axes.ravel()  # Flatten axes array for easier indexing\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    axes[idx].hist(df_prep[feature], bins=50)\n",
    "    axes[idx].set_title(f'Distribution of {feature}')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "# Remove empty subplots if any\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print basic statistics of transformed data\n",
    "print(\"\\nBasic statistics of transformed features:\")\n",
    "print(df_prep.describe().round(3))\n",
    "\n",
    "# Return the prepared DataFrame\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "print(\"CUDA available:\", cp.is_available())\n",
    "print(\"CUDA version:\", cp.cuda.runtime.runtimeGetVersion())\n",
    "\n",
    "import cuml\n",
    "print(\"cuML version:\", cuml.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup HDBSCAN with much larger min_cluster_size\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=5000,  # Significantly increased\n",
    "    min_samples=100,\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "hdbscan_model.fit(df_prep_gpu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different cut distances\n",
    "for cut_distance in [0.05, 0.1, 0.2, 0.5, 1.0]:\n",
    "    labels = hdbscan_model.single_linkage_tree_.get_clusters(\n",
    "        cut_distance=cut_distance,\n",
    "        min_cluster_size=5000  # Match the min_cluster_size from above\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCut distance: {cut_distance}\")\n",
    "    print(\"Number of clusters:\", len(np.unique(labels)))\n",
    "    print(\"Cluster sizes:\")\n",
    "    print(pd.Series(labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "# faeture engineering\n",
    "df_hdbscan = df_clean[['age_at_ed', 'los_minutes', 'heartrate', 'sbp', 'dbp', 'o2sat', 'resprate', 'temperature', 'pain', 'acuity']].copy()\n",
    "\n",
    "# setup HDBSCAN to cluster patients into groups\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=50)\n",
    "df_hdbscan['cluster'] = hdbscan_model.fit_predict(df_hdbscan)\n",
    "\n",
    "# print the number of clusters\n",
    "print('Number of clusters:')\n",
    "df_hdbscan['cluster'].nunique()\n",
    "\n",
    "# print the number of patients in each cluster\n",
    "print('Number of patients in each cluster:')\n",
    "df_hdbscan['cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract different numbers of clusters from the existing model\n",
    "for cut_distance in [0.1, 0.2, 0.3, 0.4, 0.5]:  # Try different cut distances\n",
    "    labels = hdbscan_model.single_linkage_tree_.get_clusters(\n",
    "        cut_distance=cut_distance,\n",
    "        min_cluster_size=100\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCut distance: {cut_distance}\")\n",
    "    print(\"Number of clusters:\", len(np.unique(labels)))\n",
    "    print(\"Cluster sizes:\")\n",
    "    print(pd.Series(labels).value_counts().sort_index())\n",
    "\n",
    "    # Add these labels to the dataframe for analysis if needed\n",
    "    df_hdbscan[f'cluster_dist_{cut_distance}'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature engineering\n",
    "df_hdbscan = df_clean[['age_at_ed', 'los_minutes', 'heartrate', 'sbp', 'dbp', 'o2sat', 'resprate', 'temperature', 'pain', 'acuity']].copy()\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "df_hdbscan_scaled = scaler.fit_transform(df_hdbscan)\n",
    "\n",
    "# Setup HDBSCAN with much larger min_cluster_size\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=5000,  # Significantly increased\n",
    "    min_samples=100,\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "hdbscan_model.fit(df_hdbscan_scaled)\n",
    "\n",
    "# Try different cut distances\n",
    "for cut_distance in [0.05, 0.1, 0.2, 0.5, 1.0]:\n",
    "    labels = hdbscan_model.single_linkage_tree_.get_clusters(\n",
    "        cut_distance=cut_distance,\n",
    "        min_cluster_size=5000  # Match the min_cluster_size from above\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCut distance: {cut_distance}\")\n",
    "    print(\"Number of clusters:\", len(np.unique(labels)))\n",
    "    print(\"Cluster sizes:\")\n",
    "    print(pd.Series(labels).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster labels at your preferred cut distance (e.g., 1.0)\n",
    "cut_distance = 1.0\n",
    "labels = hdbscan_model.single_linkage_tree_.get_clusters(\n",
    "    cut_distance=cut_distance,\n",
    "    min_cluster_size=5000\n",
    ")\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df_with_clusters = df_hdbscan.copy()\n",
    "df_with_clusters['cluster'] = labels\n",
    "\n",
    "# Get summary statistics for each cluster\n",
    "cluster_stats = df_with_clusters.groupby('cluster').agg({\n",
    "    'age_at_ed': ['mean', 'std', 'median'],\n",
    "    'los_minutes': ['mean', 'std', 'median'],\n",
    "    'heartrate': ['mean', 'std', 'median'],\n",
    "    'sbp': ['mean', 'std', 'median'],\n",
    "    'dbp': ['mean', 'std', 'median'],\n",
    "    'o2sat': ['mean', 'std', 'median'],\n",
    "    'resprate': ['mean', 'std', 'median'],\n",
    "    'temperature': ['mean', 'std', 'median'],\n",
    "    'pain': ['mean', 'std', 'median'],\n",
    "    'acuity': ['mean', 'std', 'median']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Cluster Statistics:\")\n",
    "print(cluster_stats)\n",
    "\n",
    "# You can also get the size of each cluster\n",
    "cluster_sizes = df_with_clusters['cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster Sizes:\")\n",
    "print(cluster_sizes)\n",
    "\n",
    "# To make it more readable, you can look at one feature at a time\n",
    "for feature in df_hdbscan.columns:\n",
    "    print(f\"\\n{feature} statistics by cluster:\")\n",
    "    print(df_with_clusters.groupby('cluster')[feature].agg(['mean', 'std', 'median']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clusters at cut_distance = 0.5\n",
    "cut_distance = 0.5\n",
    "labels = hdbscan_model.single_linkage_tree_.get_clusters(\n",
    "    cut_distance=cut_distance,\n",
    "    min_cluster_size=5000\n",
    ")\n",
    "\n",
    "# Create DataFrame with cluster labels\n",
    "df_with_clusters = df_hdbscan.copy()\n",
    "df_with_clusters['cluster'] = labels\n",
    "\n",
    "# Create box plots for each feature\n",
    "features = df_hdbscan.columns\n",
    "n_features = len(features)\n",
    "n_cols = 2  # 2 columns of plots\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    # Create boxplot\n",
    "    sns.boxplot(data=df_with_clusters, x='cluster', y=feature)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'{feature} Distribution by Cluster')\n",
    "    plt.xlabel('Cluster (-1 = outliers)')\n",
    "    plt.ylabel(feature)\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print cluster sizes for reference\n",
    "cluster_sizes = df_with_clusters['cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster Sizes:\")\n",
    "print(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GPU libraries\n",
    "from cuml.cluster import HDBSCAN\n",
    "from cuml.preprocessing import StandardScaler\n",
    "import cudf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medrecon = mimicdf.medrecon()\n",
    "medrecon.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_patient_medications(df):\n",
    "    # Group by patient ID and combine all medications across visits\n",
    "    # Filter out None/null values before creating sets\n",
    "    patient_meds = df.groupby('stay_id')['etccode'].agg(\n",
    "        lambda x: set().union(*[set([i]) for i in x if pd.notna(i)])\n",
    "    )\n",
    "    return patient_meds\n",
    "\n",
    "patient_meds = aggregate_patient_medications(medrecon)\n",
    "patient_meds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a copy and drop all rows with missing values\n",
    "df_encoded = df.copy()\n",
    "print(f\"Shape before dropping NA: {df_encoded.shape}\")\n",
    "df_encoded = df_encoded.dropna()\n",
    "print(f\"Shape after dropping NA: {df_encoded.shape}\")\n",
    "\n",
    "# Take a random sample of 5000 records\n",
    "sample_size = 15000\n",
    "if len(df_encoded) > sample_size:\n",
    "    df_encoded = df_encoded.sample(n=sample_size, random_state=42)\n",
    "    print(f\"Shape after sampling: {df_encoded.shape}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['gender', 'race', 'arrival_transport', 'disposition', 'dow']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "\n",
    "# Standardize numerical features\n",
    "numerical_cols = ['age_at_ed', 'hour', 'los_minutes', 'temperature', 'heartrate', \n",
    "                 'resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "df_tsne = tsne_model.fit_transform(df_encoded)\n",
    "df_tsne = pd.DataFrame(df_tsne, columns=['tsne1', 'tsne2'])\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=df_tsne, x='tsne1', y='tsne2', alpha=0.5)\n",
    "plt.title(f't-SNE visualization with gender (n={len(df_encoded)})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_with_contours(df_encoded, embedding, target_feature, \n",
    "                           grid_size=300, sigma=12, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Create t-SNE visualization with contour overlays for a specified feature.\n",
    "    \n",
    "    Parameters:\n",
    "        df_encoded (pd.DataFrame): Preprocessed dataframe with encoded values\n",
    "        embedding (np.array): t-SNE embedding coordinates\n",
    "        target_feature (str): Column name of the feature to visualize\n",
    "        grid_size (int): Resolution of the contour grid\n",
    "        sigma (float): Gaussian smoothing parameter\n",
    "        figsize (tuple): Figure size in inches\n",
    "    \"\"\"\n",
    "    # Get standardized values for target feature\n",
    "    scaler = StandardScaler()\n",
    "    standardized_values = scaler.fit_transform(df_encoded[[target_feature]]).ravel()\n",
    "    \n",
    "    # Create the visualization\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Base scatter plot\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "               color='gray', alpha=0.2, s=2)\n",
    "    \n",
    "    # Create grid\n",
    "    x_min, x_max = embedding[:, 0].min(), embedding[:, 0].max()\n",
    "    y_min, y_max = embedding[:, 1].min(), embedding[:, 1].max()\n",
    "    xi = np.linspace(x_min, x_max, grid_size)\n",
    "    yi = np.linspace(y_min, y_max, grid_size)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "    \n",
    "    # Interpolate using standardized values\n",
    "    zi = griddata((embedding[:, 0], embedding[:, 1]), \n",
    "                 standardized_values,\n",
    "                 (xi, yi), \n",
    "                 method='nearest',\n",
    "                 fill_value=np.nan)\n",
    "    \n",
    "    # Apply Gaussian smoothing\n",
    "    smooth_values = gaussian_filter(zi, sigma=sigma)\n",
    "    \n",
    "    # Create levels based on standard deviations\n",
    "    levels = np.linspace(-2, 2, 20)  # From -2 to +2 standard deviations\n",
    "    \n",
    "    # Plot contours\n",
    "    contourf = plt.contourf(xi, yi, smooth_values, \n",
    "                           levels=levels,\n",
    "                           cmap='RdBu_r',  # Red-Blue reversed\n",
    "                           alpha=0.5,\n",
    "                           extend='both')\n",
    "    \n",
    "    contour = plt.contour(xi, yi, smooth_values, \n",
    "                         levels=levels,\n",
    "                         colors='black',\n",
    "                         alpha=0.3,\n",
    "                         linewidths=0.5,\n",
    "                         extend='both')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contourf, \n",
    "                       label=f'{target_feature} (standardized)', \n",
    "                       orientation='vertical',\n",
    "                       fraction=0.046, \n",
    "                       pad=0.04)\n",
    "    cbar.add_lines(contour)\n",
    "    \n",
    "    plt.title(f't-SNE visualization with {target_feature} Contours (n={len(df_encoded)})')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Example usage:\n",
    "# First, create t-SNE embedding\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_embedding = tsne_model.fit_transform(df_encoded)\n",
    "\n",
    "# Then plot with different features\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'age_at_ed')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'los_minutes')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'heartrate')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'sbp')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'dbp')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'o2sat')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'resprate')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'temperature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'pain')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_with_contours(df_encoded, tsne_embedding, 'acuity')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_categorical_density(df_encoded, embedding, feature, target_value, \n",
    "                                grid_size=300, sigma=12, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Create t-SNE visualization with density contours for a categorical feature's specific value.\n",
    "    \n",
    "    Parameters:\n",
    "        df_encoded (pd.DataFrame): Preprocessed dataframe with encoded values\n",
    "        embedding (np.array): t-SNE embedding coordinates\n",
    "        feature (str): Column name of the categorical feature\n",
    "        target_value: The specific value to calculate density for (original category)\n",
    "        grid_size (int): Resolution of the contour grid\n",
    "        sigma (float): Gaussian smoothing parameter\n",
    "        figsize (tuple): Figure size in inches\n",
    "    \"\"\"\n",
    "    # Get original values from df_encoded's index to ensure matching lengths\n",
    "    original_values = df.loc[df_encoded.index, feature]\n",
    "    \n",
    "    # Create binary indicator for target value using original values\n",
    "    binary_indicator = (original_values == target_value).astype(float)\n",
    "    \n",
    "    # Create the visualization\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Base scatter plot\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "               color='gray', alpha=0.2, s=2)\n",
    "    \n",
    "    # Create grid\n",
    "    x_min, x_max = embedding[:, 0].min(), embedding[:, 0].max()\n",
    "    y_min, y_max = embedding[:, 1].min(), embedding[:, 1].max()\n",
    "    xi = np.linspace(x_min, x_max, grid_size)\n",
    "    yi = np.linspace(y_min, y_max, grid_size)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "    \n",
    "    # Interpolate density values\n",
    "    zi = griddata((embedding[:, 0], embedding[:, 1]), \n",
    "                 binary_indicator,\n",
    "                 (xi, yi), \n",
    "                 method='nearest',\n",
    "                 fill_value=np.nan)\n",
    "    \n",
    "    # Apply Gaussian smoothing\n",
    "    smooth_density = gaussian_filter(zi, sigma=sigma)\n",
    "    \n",
    "    # Create levels for density (0 to 1)\n",
    "    levels = np.linspace(0, 1, 20)\n",
    "    \n",
    "    # Plot contours\n",
    "    contourf = plt.contourf(xi, yi, smooth_density, \n",
    "                           levels=levels,\n",
    "                           cmap='RdBu_r',  # Red-Blue reversed\n",
    "                           alpha=0.5,\n",
    "                           extend='both')\n",
    "    \n",
    "    contour = plt.contour(xi, yi, smooth_density, \n",
    "                         levels=levels,\n",
    "                         colors='black',\n",
    "                         alpha=0.3,\n",
    "                         linewidths=0.5,\n",
    "                         extend='both')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contourf, \n",
    "                       label=f'Density of {feature}={target_value}', \n",
    "                       orientation='vertical',\n",
    "                       fraction=0.046, \n",
    "                       pad=0.04)\n",
    "    cbar.add_lines(contour)\n",
    "    \n",
    "    plt.title(f't-SNE visualization: Density of {feature}={target_value} (n={len(df_encoded)})')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Plot density of patients discharged home\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'disposition', 'HOME')\n",
    "plt.show()\n",
    "\n",
    "# Plot density of patients discharged to home\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'disposition', 'ADMITTED')\n",
    "plt.show()\n",
    "\n",
    "# Plot density of patients discharged to home\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'disposition', 'OTHER')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'arrival_transport', 'WALK IN')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'arrival_transport', 'AMBULANCE')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'arrival_transport', 'OTHER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot density of female patients\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'gender', 'F')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'gender', 'M')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'race', 'WHITE')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'race', 'BLACK')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'race', 'ASIAN')\n",
    "plt.show()\n",
    "\n",
    "plot_tsne_categorical_density(df_encoded, tsne_embedding, 'race', 'HISPANIC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a copy and drop all rows with missing values\n",
    "df_encoded = df.copy()\n",
    "print(f\"Shape before dropping NA: {df_encoded.shape}\")\n",
    "df_encoded = df_encoded.dropna()\n",
    "print(f\"Shape after dropping NA: {df_encoded.shape}\")\n",
    "\n",
    "# Take a random sample\n",
    "sample_size = 5000\n",
    "if len(df_encoded) > sample_size:\n",
    "    df_encoded = df_encoded.sample(n=sample_size, random_state=42)\n",
    "    print(f\"Shape after sampling: {df_encoded.shape}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['gender', 'race', 'arrival_transport', 'disposition', 'dow']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "\n",
    "# Standardize numerical features\n",
    "numerical_cols = ['age_at_ed', 'hour', 'los_minutes', 'temperature', 'heartrate', \n",
    "                 'resprate', 'o2sat', 'sbp', 'dbp', 'pain', 'acuity']\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "df_tsne = pd.DataFrame(\n",
    "    tsne_model.fit_transform(df_encoded), \n",
    "    columns=['tsne1', 'tsne2']\n",
    ")\n",
    "\n",
    "# Apply UMAP\n",
    "umap_model = umap.UMAP(random_state=42)\n",
    "df_umap = pd.DataFrame(\n",
    "    umap_model.fit_transform(df_encoded),\n",
    "    columns=['umap1', 'umap2']\n",
    ")\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# t-SNE plot\n",
    "sns.scatterplot(data=df_tsne, x='tsne1', y='tsne2', alpha=0.5, ax=ax1)\n",
    "ax1.set_title(f't-SNE visualization (n={len(df_encoded)})')\n",
    "\n",
    "# UMAP plot\n",
    "sns.scatterplot(data=df_umap, x='umap1', y='umap2', alpha=0.5, ax=ax2)\n",
    "ax2.set_title(f'UMAP visualization (n={len(df_encoded)})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_with_contours(df_encoded, embedding_df, target_feature, \n",
    "                           grid_size=300, sigma=12, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Create UMAP visualization with contour overlays for a specified feature.\n",
    "    \n",
    "    Parameters:\n",
    "        df_encoded (pd.DataFrame): Preprocessed dataframe with encoded values\n",
    "        embedding_df (pd.DataFrame): UMAP embedding coordinates DataFrame\n",
    "        target_feature (str): Column name of the feature to visualize\n",
    "        grid_size (int): Resolution of the contour grid\n",
    "        sigma (float): Gaussian smoothing parameter\n",
    "        figsize (tuple): Figure size in inches\n",
    "    \"\"\"\n",
    "    # Convert embedding DataFrame to numpy array\n",
    "    embedding = embedding_df.values\n",
    "    \n",
    "    # Get standardized values for target feature\n",
    "    scaler = StandardScaler()\n",
    "    standardized_values = scaler.fit_transform(df_encoded[[target_feature]]).ravel()\n",
    "    \n",
    "    # Create the visualization\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Base scatter plot\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "               color='gray', alpha=0.2, s=2)\n",
    "    \n",
    "    # Create grid\n",
    "    x_min, x_max = embedding[:, 0].min(), embedding[:, 0].max()\n",
    "    y_min, y_max = embedding[:, 1].min(), embedding[:, 1].max()\n",
    "    xi = np.linspace(x_min, x_max, grid_size)\n",
    "    yi = np.linspace(y_min, y_max, grid_size)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "    \n",
    "    # Interpolate using standardized values\n",
    "    zi = griddata((embedding[:, 0], embedding[:, 1]), \n",
    "                 standardized_values,\n",
    "                 (xi, yi), \n",
    "                 method='nearest',\n",
    "                 fill_value=np.nan)\n",
    "    \n",
    "    # Apply Gaussian smoothing\n",
    "    smooth_values = gaussian_filter(zi, sigma=sigma)\n",
    "    \n",
    "    # Create levels based on standard deviations\n",
    "    levels = np.linspace(-2, 2, 20)  # From -2 to +2 standard deviations\n",
    "    \n",
    "    # Plot contours\n",
    "    contourf = plt.contourf(xi, yi, smooth_values, \n",
    "                           levels=levels,\n",
    "                           cmap='RdBu_r',  # Red-Blue reversed\n",
    "                           alpha=0.5,\n",
    "                           extend='both')\n",
    "    \n",
    "    contour = plt.contour(xi, yi, smooth_values, \n",
    "                         levels=levels,\n",
    "                         colors='black',\n",
    "                         alpha=0.3,\n",
    "                         linewidths=0.5,\n",
    "                         extend='both')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contourf, \n",
    "                       label=f'{target_feature} (standardized)', \n",
    "                       orientation='vertical',\n",
    "                       fraction=0.046, \n",
    "                       pad=0.04)\n",
    "    cbar.add_lines(contour)\n",
    "    \n",
    "    plt.title(f'UMAP visualization with {target_feature} Contours (n={len(df_encoded)})')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_with_contours(df_encoded, df_umap, 'age_at_ed')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'los_minutes')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'heartrate')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'sbp')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'dbp')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'o2sat')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'resprate')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'temperature')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'pain')\n",
    "plt.show()\n",
    "\n",
    "plot_umap_with_contours(df_encoded, df_umap, 'acuity')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple UMAP visualizations with different parameters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "# Different parameter combinations\n",
    "params = [\n",
    "    {'n_neighbors': 25, 'min_dist': 0.1, 'title': 'Default (n=15, dist=0.1)'},\n",
    "    {'n_neighbors': 25, 'min_dist': 0.25, 'title': 'More Clustered (n=5, dist=0.0)'},\n",
    "    {'n_neighbors': 25, 'min_dist': 0.50, 'title': 'More Spread (n=30, dist=0.5)'},\n",
    "    {'n_neighbors': 25, 'min_dist': 0.85, 'title': 'Most Global (n=50, dist=0.8)'}\n",
    "]\n",
    "\n",
    "for (i, j), param in zip([(0,0), (0,1), (1,0), (1,1)], params):\n",
    "    # Create UMAP embedding\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=param['n_neighbors'],\n",
    "        min_dist=param['min_dist'],\n",
    "        random_state=42\n",
    "    )\n",
    "    embedding = umap_model.fit_transform(df_encoded)\n",
    "    \n",
    "    # Plot\n",
    "    axes[i,j].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, s=2)\n",
    "    axes[i,j].set_title(param['title'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimiced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
